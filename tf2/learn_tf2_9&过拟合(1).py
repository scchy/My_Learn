# python3.6
# Create date: 2020-05-25
# Function: 19年11月最新-《TensorFlow+2.0深度学习算法实战教材》

import tensorflow as tf 
import math
import abc
import numpy as np

# ======== 目录 ==========
# &9 过拟合
#   - 9.1 模型的容量
#   - 9.2 过拟合与欠拟合
#   - 9.3 数据集划分
#   - 9.5 正则化
#   - 9.6 Dropout
#   - 9.7 数据增强
# ========================

# =======================================================================================================
#                                           第九章   过拟合
# =======================================================================================================

# 9.1 模型的容量
# ---------------------------------------------------
"""
通俗的讲， 模型的容量或表达能力，是指模型拟合复杂函数的能力。一种体现模型容量的指标为
模型的假设空间(Hypothesis Space)大小，即模型可以表示的函数的大小。
"""
# 9.2 过拟合与欠拟合
# ---------------------------------------------------
"""
那么如何有效检测并减少过拟合现象
呢？接下来我们将介绍一系列的方法，来帮我们检测并抑制过拟合现象

"""

# 9.3 数据集划分
# ---------------------------------------------------
# 9.3.1 验证集与超参数
"""
由于测试集的性能不能作为模型训练
的反馈，而我们需要在模型训练时能够挑选出较合适的模型超参数，判断模型是否过拟合
等:
    - 根据验证集的性能表现来调整学习率，权值衰减系数，训练次数等
    - 根据验证集的性能表现来重新调整网络拓扑结构
    - 根据验证集的性能表现判断是否过拟合和欠拟合

训练集-测试集的划分类似，训练集、验证集和测试集可以按着自定义的比例来划分，比
如常见的60%-20%-20%的划分
验证集与测试集的区别在于，算法设计人员可以根据验证集的表现来调整模型的各种
超参数的设置，提升模型的泛化能力，但是测试集的表现却不能用来反馈模型的调整，否
则测试集将和验证集的功能重合，因此在测试集上面的性能表现将无法代表模型的泛化能
力。
"""
# 9.3.2 提前停止
"""
一个batch更新一次叫一个step, 对训练集的所有样本循环迭代一次叫一个epoch。
验证集可以在数次Step 或数次Epoch 后使用。

通过观测训练准确率和验证准确率可以大致推断模型是否过拟合和欠拟合。如果
模型的训练误差较低，训练准确率较高，但是验证误差较高，验证准确率较低，那么可能
出现了过拟合现象。
- 考虑降低网络的层数，降低网络的参数量、添加假设空间的约束等
- 也可以通过稀疏化参数、添加正则化等手段降
- early stop 我们可以记录模型的验证准确率，并监控验证准确率的变化，当发现验证准确率连
续P 个Epoch 没有下降时，可以预测已经达到了最适合的epoch 附近，从而提前终止训

如果训练集和验证集上面的误差都较高，准确率较低，那么可能出现
了欠拟合现象

"""

# 9.4 模型设计
# ---------------------------------------------------
# 9.5 正则化
# ---------------------------------------------------
"""
新的优化目标除了要最小化原来的损失函数ℒ( , 𝑦)之外，还需要约束网络参数的稀疏性，
优化算法会在降低ℒ( , 𝑦)的同时，尽可能地迫使网络参数𝜃𝑖变得稀疏，他们之间的权重关
系通过超参数𝜆来平衡，
- 较大的 𝜆 意味着网络的稀疏性更重要；
- 较小的 𝜆 则意味着网络的训练误差更重要

"""
## 9.5.1 L0
#  𝜃𝑖中非零元素的个数

## 9.5.1 L1
#  𝜃𝑖中所有元素的绝对值之和 Lasso Regularization
### L1 正则可以实现如下:
import tensorflow as tf 
w1 = tf.random.normal([4, 3])
w2 = tf.random.normal([4, 2])
loss_reg = tf.reduce_sum(tf.math.abs(w1)) + tf.reduce_sum(tf.math.abs(w2))

## 9.5.1 L2
#  𝜃𝑖中所有元素的平方和 Ridge Regularization
loss_reg = tf.reduce_sum(tf.math.square(w1)) + tf.reduce_sum(tf.math.square(w2))


# 9.6 Dropout
# ---------------------------------------------------
"""
。Dropout 通过随机断
开神经网络的连接，减少每次训练时实际参与计算的模型的参数量；但是在测试时，
Dropout 会恢复所有的连接，保证模型测试时获得最好的性能。
"""
import tensorflow as tf
x = tf.nn.dropout(x, rate=0.5)
# model.add(layers.Dropout(rate=0.5)

# 9.7 数据增强
# ---------------------------------------------------
"""
增加数据集大小是解决过拟合最重要的途径。
以图片数据为例，我们来介绍怎么做数据增强

对于图中的人物图片，根据先验知识，我们知道旋
转、缩放、平移、裁剪、改变视角、遮挡某局部区域都不会改变图片的类别标签，因此针
对图片数据，我们可以有多种数据增强方式
"""
## tf.image 子模块
def prepreocess(x, y):
    # x: 图片的路径， y:图片的数字编码
    x = tf.io.read_file(x)
    x = tf.image.decode_jpeg(x, channels=3) # RGBA
    # 图片缩放到 244*244 大小，这个大小根据网络设定自行调整
    x = tf.image.resize(x, [244, 244])

## 9.7.1 旋转
### 图片选择180度
    x = tf.image.rot90(x, k=2)
## 9.7.2 翻转
    x = tf.image.random_flip_left_right(x) # 水平
    x = tf.image.random_flip_up_down(x)    # 上下
## 9.7.3 裁剪
    x = tf.image.rsize(x, [244, 244])
    ## 再随机裁剪到合适尺寸
    x = tf.image.random_crop(x, [244, 244, 3])
## 9.7.4 生成数据
"""
通过生成模型在原有数据上学习到数据的分布， 从而生成新的样本，这种方式也可以在一定程度上提升网络的性能。
如通过条件生成对抗网络（Conditional GAN)可以生成带标签的样本数据
"""
## 9.7.5 其他方式
"""
添加高斯噪声
变换视角
随机擦除
"""
