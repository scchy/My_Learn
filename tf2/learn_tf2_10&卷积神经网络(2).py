# python3.6
# Create date: 2020-06-01
# Function: 19年11月最新-《TensorFlow+2.0深度学习算法实战教材》

import tensorflow as tf 
import math
import numpy as np

# ======== 目录 ==========
# &10 卷积神经网络
#   - 10.5 表示学习
#   - 10.6 梯度传递
#   - 10.7 池化层
#   - 10.8 BatchNorm层
# ========================

# =======================================================================================================
#                                           第十章   卷积神经网络
# =======================================================================================================

# 10.5 表示学习
# -------------------------------------------------
import tensorflow as tf
"""
图片数据的识别过程一般也是表示学习的过程, 从接受到原始像素开始，逐渐提取边缘、角点等底层特征。
再到纹理等中层特征，再到头部、物体部件等高层特征，最后的网络层基于这些学习到的抽象特征表示作为分类逻辑的学习
学习到的特征越高层，越准确，就越有利于分类器的分类，从而获得较好的
性能。从表示学习的角度来理解，卷积神经网络通过层层堆叠来逐层提取特征，网络训练
的过程可以看成特征的学习过程，基于学习到的高层抽象特征可以方便地进行分类任务


应用表示学习的思想，训练好的卷积神经网络往往能够学习到较好的特征，这种特征
的提取方法一般是通用的。比如在猫、狗任务上学习到头、脚、身躯等特征的表示，在其
他动物上也能够一定程度上使用。基于这种思想，可以将在任务A 上训练好的深层神经网
络的前面数个特征提取层迁移到任务B 上，只需要训练任务B 的分类逻辑(表现为网络的
最末数层)，即可取得非常好的效果，这种方式是迁移学习的一种，从神经网络角度也称为
网络微调(Fine-tuning)。

"""
# 10.6 梯度传递
# -------------------------------------------------
# p237 

# 10.7 池化层
# -------------------------------------------------
"""
池化层同样基于局部相关性的思想， 通过从局部相关的一组元素中进行采样或信息聚合，从而得到新的元素。
以5*5输入X的最大池化层为例，考虑池化感受野窗口大小k=2, 步长s=1的情况。

o = [
[1, -1, 0, 2, 0],
[-1, -2, 2, 3, 1],
[1, 2, -2, 1, 0],
[0, -1, -1, -3, 2],
[2, 0, 0, 1, -1]
]
pool_window.shape=(2,2)
# fst
o = [
[【1, - 1, 】  0, 2, 0],
[【-1, -2, 】  2, 3, 1],
[1, 2, -2, 1, 0],
[0, -1, -1, -3, 2],
[2, 0, 0, 1, -1]
] -->> 1
o = [
[1,  【- 1,  0】, 2, 0],
[-1, 【-2,   2】, 3, 1],
[1, 2, -2, 1, 0],
[0, -1, -1, -3, 2],
[2, 0, 0, 1, -1]
] -->> 2

k=2 s=2 可以将输出高宽减少一半
"""

# 10.8 BatchNorm层
# -------------------------------------------------
"""
在残差网络出现之前，网络的加深使得网络训练变得非常不稳定，设置出现网络长时间不更新
或者不收敛的情形，同时网络对超参数比较敏感，超参数的为扰动也会导致网络的训练轨迹完全
改变。
2015年提出一种参数标准化的手段，使得网络的超参数设定更加自由，比如更大的学习率，更随意的
网络初始化等，同时网络的收敛更快，性能也更好。
BN层提出后便广泛的应用在各种深度网络模型上，卷积-BN-ReLU-Pooling一度成为网络模型的标配单元
"""

# 10.8.1 前向传播
"""
𝛾参数实现对标准化后的𝑥̂再次进行缩放，𝛽参数实现对标准化的𝑥̂进行平移

1- 计算当前Batch的 u_B， sigma_B^2 根据
x_tr' = (x_tr - u_B)/(sqrt(sigma_B^2 + e)) * r  + belta 

u_r <- momentum*u_r + (1-momentum)*u_B

TensorFlow 中，momentum 默认设置为0.99

"""
# 10.8.2 反向更新
x=tf.random.normal([100, 32, 32, 3])
x = tf.reshape(x, [-1, 3])
# 计算每个维度 均值
ub = tf.reduce_mean(x, axis=0) # C个通道就有C个均值
"""
Layer Norm 统计每个样本的所有特征的均值和方差
Instance Norme 统计每个样本的每个通道上特征的均值和方差
Group Norm: 将C通道分成若干组，每个统计样本的通道组内的特征均值和方差


"""
# 10.8.3 BN层实现
from tensorflow.keras import layers, Sequential
layer = layers.BatchNormalization()
network = Sequential([
    layers.Conv2D(6, kernel_size=3, strides=1),
    # 插入BN层
    layers.BatchNormalization(),
    layers.MaxPooling2D(pool_size=2, strides=2),
    layers.ReLU(),

    layers.Conv2D(16,kernel_size=3,strides=1),
    # 插入BN 层
    layers.BatchNormalization(),
    layers.MaxPooling2D(pool_size=2,strides=2),
    layers.ReLU(),
    layers.Flatten(),

    layers.Dense(120, activation='relu'),
    # 此处也可以插入BN 层
    layers.Dense(84, activation='relu'),
    # 此处也可以插入BN 层
    layers.Dense(10)
])
# 需要设置网络的参数training=True 以区分BN 层是训练还是测试模型
with tf.GradientTape() as tape:
    x = tf.expand_dims(x, axis=3)
    #  前向计算，设置计算没事, 
    out = network(x, training=True)




